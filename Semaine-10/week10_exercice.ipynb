{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 10 les reseaux de neurone\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "\n",
    "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "\n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
    "\n",
    "Visually, if we omit the \"pixel\" prefix, the pixels make up the image like this:\n",
    "\n",
    "000 001 002 003 ... 026 027\n",
    "028 029 030 031 ... 054 055\n",
    "056 057 058 059 ... 082 083\n",
    " |   |   |   |  ...  |   |\n",
    "728 729 730 731 ... 754 755\n",
    "756 757 758 759 ... 782 783 \n",
    "The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## Creer un Neural Network - Binary Classification\n",
    "\n",
    "Dans cet exercice, nous allons réaliser une classification binaire entre les chiffres 0 et 1  du data set mnist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "abf6aa77-17cd-4b4a-bacf-903838fe813e",
    "_uuid": "735f9f4ab4893a28cc6a28f81a49a182fd02747d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e249a1aa-a3df-4342-97ea-48a380d642a0",
    "_uuid": "3fa615d978d15f0430a3adf0bbe2b7ccc8205f5f"
   },
   "source": [
    "### 1 Dataset Preparation\n",
    "\n",
    "Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "34aa89c5-b5b7-4b34-9f16-0af4a4046cdf",
    "_uuid": "670c2f126fa04d98426502d720a6288e5b0cb5df"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-51c57cff01ce>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-51c57cff01ce>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    X =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"src/train.csv\")\n",
    "\n",
    "# Inclure seulement les chiffres avec les labels 0 et 1\n",
    "# (man .isin)\n",
    "X = \n",
    "\n",
    "# Supprimer la colonne label\n",
    "X = \n",
    "\n",
    "# construisons Y a partir de la colone label\n",
    "Y = train[train['label'].isin([0, 1])]['label']\n",
    "Y = Y[:,np.newaxis]\n",
    "\n",
    "# on transpose X et Y pour etre au format colone\n",
    "Y = Y.T\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons ce que ca donne !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va afficher quelques exemples des images sur lesquelles vous allez travailler\n",
    "\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "columns = 4\n",
    "rows = 5\n",
    "for i in range(1, columns*rows +1):\n",
    "    X1 = X.T.iloc[i]\n",
    "    X1 = X1.values.reshape(28,28)\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(X1, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X\", X.shape)\n",
    "print(\"Y\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats\n",
    "\n",
    "Les resultats devraient etre pour X et Y devraient être deux matrices de dimensions respectives\n",
    "\n",
    "X (784, 8816) <br>\n",
    "Y (1, 8816)\n",
    "\n",
    "784 etant le nombre d'input (chaques pixels allant de 0 a 255 en intensité lumineuse) <br>\n",
    "8816 le nombre de training examples. <br>\n",
    "On retrouve ici les matrices en colones plutot qu'en ligne <br>\n",
    "Si on print Y on obtient une serie de 0 et de 1 (le label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "378fd937-d372-4eab-bcfd-46bf5817ed01",
    "_uuid": "37ea4b7a6bbedd53b00dec3cfefad523b44d68fb"
   },
   "source": [
    "### 2 Fonction d'activation\n",
    "\n",
    "We will use sigmoid activation function because it outputs the values between 0 and 1 so its a good choice for a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19fc3ff1-f6ac-447d-98ea-4ff4d4bf4669",
    "_uuid": "13f0338a967a4cb87503821f044c09552d2e1640"
   },
   "outputs": [],
   "source": [
    "# Si 'derivative' == True, alors retourner le resultat de la fonction derivée\n",
    "def sigmoid(x, derivative=False):\n",
    "    if (derative = False)\n",
    "        return\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats\n",
    "\n",
    "On peut verifier son resultat avec les valeurs connues de la fontion sigmoid comme : <br>\n",
    "sigmoid(0) = 0.5 <br>\n",
    "sigmoid(0, derivative = True) = 0.25 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigmoid(0))\n",
    "print(sigmoid(0, derivative = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee1e1ebe-4298-40cd-9044-d8dc4327aa6c",
    "_uuid": "3c58c24a7feb9789a5bd84e8c9cc5068eebbee04"
   },
   "source": [
    "### 3 Defirnir l'Architecture de notre NN\n",
    "\n",
    "Nous voulons creer ici une architecture pour notre reseau de neurone : combien de layers ? d'units pour chaque layer ect ... <br>\n",
    "<br>\n",
    "Nous commencerons par un reseau a 2 layers : 1 input layer, 1 hidden layer et 1 output layer : <br>\n",
    " - Un input de 784 (le nombre de feature pour chaque image) <br>\n",
    " - 10 unités dans le hidden layer <br>\n",
    " - Seulement une unité dans l'output layer qui vaudra zero si notre image represente un 0 et ... 1 si c'est un 1 !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a4c98dcc-6a34-4e5d-99d7-7e26ec12ce21",
    "_uuid": "deab1ad6439f304b669f70da3ca1f70e13f027f6"
   },
   "outputs": [],
   "source": [
    "def network_architecture(X, Y):\n",
    "    # nodes in input layer\n",
    "    n_x = X.shape[0]\n",
    "    # nodes in hidden layer\n",
    "    n_h = 10         \n",
    "    # nodes in output layer\n",
    "    n_y = Y.shape[0]\n",
    "    return (n_x, n_h, n_y)\n",
    "\n",
    "n_x, n_h, n_y = network_architecture(X, Y)\n",
    "print (\"n_x\", n_x)\n",
    "print (\"n_h\", n_h)\n",
    "print (\"n_y\", n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc75a514-66e5-4479-9613-7fdc4572271d",
    "_uuid": "c91f07b7c28808b74babea3d013fbfdd6ea3ba91"
   },
   "source": [
    "### 4 Definir les parametres de notre reseau de neurone\n",
    "\n",
    "Afin d'initializer aleatoirement nos poids on va utiliser une fonction de numpy qui initialise une matrice de dimension de notre choix avec des nombres issues d'une distribution normale que l'on mettra a l'echelle 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cef24778-cd2c-4412-b017-7f4d9b34f34a",
    "_uuid": "c13e2d5cd4d04c05c878f7ccab1677b290847be4"
   },
   "outputs": [],
   "source": [
    "def define_network_parameters(n_x, n_h, n_y):\n",
    "    W1 = # random initialization between inputs and node of hidden layer, remettre à l'echelle en multipliant par 0.01\n",
    "    b1 = # zeros initialization\n",
    "    W2 = # random initialization, remettre à l'echelle en multipliant par 0.01\n",
    "    b2 = # zero initialization\n",
    "\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On devrait avoir les dimensions suivantes : <br>\n",
    "w1 (10, 784)<br>\n",
    "b1 (10, 1)<br>\n",
    "w2 (1, 10)<br>\n",
    "b2 (1, 1)\n",
    "\n",
    "\n",
    "Et puis ca ne fait jamais de mal de visualiser de temps en temps ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = network_architecture(X, Y)\n",
    "\n",
    "params = define_network_parameters(n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd1fd5e9-aae9-4996-8137-6f577147b90c",
    "_uuid": "9664d29beedd6c5a7c27b10201992a3c2f810c56"
   },
   "source": [
    "### 5 Forward Propagation\n",
    "\n",
    "Pour rappel :\n",
    "    Z = W*X + b </br>\n",
    "    A = g(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2fd5cfe1-dbe2-4580-b541-1ed5cf139166",
    "_uuid": "ee7d22c45eeb9768b2722c36ecf9f73ff9ef2db9"
   },
   "outputs": [],
   "source": [
    "# Pour rappel Z = W*X + b   & A = g(Z)\n",
    "\n",
    "def forward_propagation(X, params):\n",
    "    Z1 = \n",
    "    A1 = \n",
    "\n",
    "    Z2 = \n",
    "    A2 = \n",
    "    \n",
    "    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats\n",
    "\n",
    "On peut au moins verifier la forme des matrices calculées : <br>\n",
    "Z1  (10, 8816)<br>\n",
    "A1  (10, 8816)<br>\n",
    "Z2  (1, 8816) <br>\n",
    "A2  (1, 8816) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refor = forward_propagation(X, params)\n",
    "\n",
    "print(\"Z1\", refor['Z1'].shape)\n",
    "print(\"A1\", refor['A1'].shape)\n",
    "print(\"Z2\", refor['Z2'].shape)\n",
    "print(\"A2\", refor['A2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c4ef52b-55ac-4ef2-a5fd-ee164db8dcdc",
    "_uuid": "58d61c5257c8f568d439a605fc311f483f09f1e2"
   },
   "source": [
    "### 6 Backward Propagation\n",
    "\n",
    "In backward propagation function, the error is passed backward to previous layers and the derivatives of weights and bias are computed. The weights and bias are then updated using the derivatives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8796bbb2-8fbf-4740-9687-c3ec2c38a27f",
    "_uuid": "195d2b62d4655b7ecff22e4bc70be15256aadf1b"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(params, activations, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # output layer\n",
    "    dZ2 =  # compute the error derivative \n",
    "    dW2 =  # compute the weight derivative \n",
    "    db2 =  # compute the bias derivative\n",
    "    \n",
    "    \n",
    "    # hidden layer, la meme chose avec une nouvelle methode de calcul pour dZ1\n",
    "    \n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "def update_parameters(params, derivatives, alpha = 1):\n",
    "    # alpha is the model's learning rate \n",
    "    \n",
    "    params['W1'] = #tout le monde se rappel de la descente des gradients ;) ?\n",
    "    params['b1'] = \n",
    "    params['W2'] = \n",
    "    params['b2'] = \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois la meilleure verification consiste a verifier ses dimensions et a jeter un oeuil a son resultat :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = update_parameters(params, backward_propagation(params, refor, X, Y))\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "155f019d-3e33-4cdd-9722-4d0a7121276f",
    "_uuid": "08d7b796a3dbdcb1530053238037803ac76362c3"
   },
   "outputs": [],
   "source": [
    "def compute_error(Predicted, Actual):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "935845e8-c35f-43ae-8f28-65eee43fc428",
    "_uuid": "1724d52016b387a2f974e6c6fc27c2a17a96166b"
   },
   "source": [
    "### 7 Compile et entrainner le Model\n",
    "\n",
    "Create a function which compiles all the key functions and creates a neural network model.\n",
    "\n",
    "Maintenant que l'on a poser une a une les pierres de notre algorythme il est temps de prendre un peu de recul pour verifier que tout s'articule et fonctionne de maniere harmonieuse : <br>\n",
    " - Il faut definir l'architecture de notre reseau\n",
    " - initialiser nos parametres\n",
    " - puis pour un nombre donné d'époches on fait une prediction de resultat puis on affine nos parametres (gradient descent) ... notre algorythme \"apprend\" <br>\n",
    " ### Optionnel\n",
    " Vous pouvez egalement creer une fonction qui permet de calculer le cout à chaque iteration, cela vous permettra d'etre sur que vous etre bien arrivé à converger.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "84c99459-755e-4795-a52f-bb8cf59299a4",
    "_uuid": "011debd87ff12ce75d29dac9383d74db9c64a35a"
   },
   "outputs": [],
   "source": [
    "def neural_network(X, Y, num_iterations):\n",
    "    #on definit l'architecture de notre reseau\n",
    "    \n",
    "    #on en profite pour initialiser aleatoirement nos parametres\n",
    "    \n",
    "    #puis on boucle\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se lance maintenant !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb2fa7af-e787-4ca1-9079-f6dfb6723390",
    "_uuid": "2d983cbb3c77270b2ee66d9148315cb24888f292"
   },
   "outputs": [],
   "source": [
    "model = neural_network(X, Y, num_iterations = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6dccfd5a-8850-41a0-b406-47460f5bd2ed",
    "_uuid": "dc33c60645cdaba20c5a420ab6bc7e5933b6e88b"
   },
   "source": [
    "### 8 Predictions \n",
    "\n",
    "La fonction predict va nous permettre a partir de nos parametres fraichement affinés de formuler avec une n-ième forward-propagation la fiabilité de nos predictions sur notre training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "59eed767-201d-4138-a77e-2ce8eaad83df",
    "_uuid": "df27eb0a992e1d7646c2ef5f5c7ca928bca6155e"
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    results = forward_propagation(X, parameters)\n",
    "    predictions = np.around(results['A2'])    \n",
    "    return predictions\n",
    "\n",
    "predictions = predict(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error va contenir l'index (sur nos 8860 training examples) des exemples ou notre algorithme s'est trompé\n",
    "\n",
    "Error = []\n",
    "\n",
    "for i in range(Y.shape[1]):\n",
    "    #on remplit error\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Accuracy: %f' % float((float(X.shape[0]) - float(len(Error))) / float(X.shape[0]) * 100) + '%')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 Visualisations \n",
    "\n",
    "alors c'etait fun ? On s'est bien amusés a travailler cet atelier et on espere que vous aussi. Merci pour votre patience et votre investissement avant de vous laisser, voila la visualisation des chiffres que vous reseau de neurone à mal classifié !<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8, 8))\n",
    "columns = 4\n",
    "rows = 5\n",
    "for i in range(1, len(Error)+1):\n",
    "    \n",
    "    X1 = X.T.iloc[Error[i-1]]\n",
    "    X1 = X1.values.reshape(28,28)\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(X1, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "\n",
    "Pleins de possibilite d'amelioration\n",
    "- Permettre une classification multivariable\n",
    "- Permettre de rajouter plusieurs Layers\n",
    "- Decoupter notre dataset pour faire de la cross validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
