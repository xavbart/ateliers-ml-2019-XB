{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 13 - Reinforcement Learning, Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exercice de cette semaine, nous utiliserons le module *Gym* d'OpenAI. Gym implémente divers environnements compatibles avec des problèmes de RL pour permettre à la communauté scientifique de s'y entraîner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le lien vers la documentation de *Gym*:   https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans MountainCar, il faire à apprendre au pilote de la voiture comment monter au sommet de la montagne. Mais le moteur n'a pas assez de puissance pour y arriver d'un coup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for timestep in range(200):\n",
    "    env.render()\n",
    "    env.step(2)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pilote dispose de 3 actions possibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez différentes valeurs pour la variable \"action\" afin de découvrir comment est encodé le choix de l'action pour le passer à l'environnement au travers de la méthode step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "for timestep in range(200):\n",
    "    env.render()\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les observations et les états"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'environnement fournit à l'agent deux valeurs continues comme observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une des valeurs correspond à la position, l'autre à la vitesse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les observations sont renvoyées par deux différentes méthodes de l'environnement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "print(observation)\n",
    "observation, reward, done, info = env.step(action)\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour faire du q-learning, il faut un nombre fini d'état, donc nos valeurs continues doivent devenir discrètes. Écrivez une fonction qui divise l'échelle de variation de chaque valeur en 20 segments égaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_segments = 20\n",
    "\n",
    "def obs_to_state(obs):\n",
    "    min_position, min_speed = env.observation_space.low\n",
    "    max_position, max_speed = env.observation_space.high\n",
    "    \n",
    "    \"\"\"À compléter\"\"\"\n",
    "    \n",
    "    position_segm = \n",
    "    speed_segm = \n",
    "    \n",
    "    state = position_segm, speed_segm\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez votre fonction: Faites-lui transformer l'état renvoyé par env.reset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par souci de simplicité, nous n'allons pas implémenter l'agent comme objet, mais néanmoins nous implémenterons sa Policy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez une table de q-valeurs qui peut contenir une q-valeur distincte pour chaque action possible, dans tous les états imaginables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dimensions = \n",
    "q_table = np.random.uniform(low=-1, high=1, size=dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez la Greedy policy, qui utilise la q_table pour décider de la meilleure action à choisir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state):\n",
    "    action = \"\"\"À compléter\"\"\"\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez maintenant la Epsilon-greedy policy, qui ajoute une probabilité *epsilon* que l'action soit choisie aléatoirement  \n",
    "_**Indice:**_ np.random.choice(n) choisira aléatoirement un nombre de 0 à n exclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.rand(1) < epsilon:\n",
    "        action = \"\"\"À compléter\"\"\"\n",
    "    else:\n",
    "        action = \"\"\"À compléter\"\"\"\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "episodes_max = 5000\n",
    "lr = 0.2\n",
    "gamma = 0.9\n",
    "\n",
    "# Epsilon will diminish over the episodes to increase exploitation\n",
    "start_epsilon = 0.8\n",
    "final_epsilon = 0\n",
    "epsilon_reduction = (start_epsilon - final_epsilon) / episodes_max\n",
    "epsilon = start_epsilon\n",
    "\n",
    "reward_list = []\n",
    "avg_reward_list = []\n",
    "\n",
    "# Train for 'episodes_max' episodes\n",
    "for episode in range(episodes_max):\n",
    "    \n",
    "    # Initialize parameters of the episode\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    state1 =  \"\"\"A compléter\"\"\"\n",
    "    \"\"\"N'oubliez pas de transformer ce que retourne l'environment en valeurs discrètes!\"\"\"\n",
    "    \n",
    "    # Each episode runs for a maximum of 200 timesteps, at whith point the Environment returns done = True\n",
    "    while done is False:\n",
    "        \n",
    "        # The agent chooses an action\n",
    "        action = \"\"\"À compléter\"\"\"\n",
    "        \n",
    "        # The action is given to the environment, which returns a new state, a reward, and some other info\n",
    "        # (see documentation https://gym.openai.com/docs/)\n",
    "        _, _, _, _ = env.\"\"\"À compléter\"\"\"\n",
    "        \n",
    "        state2 = \"\"\"À compléter\"\"\"\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update q_table\n",
    "        if done:\n",
    "            \"\"\"Quelle est la q-valeur de la toute dernière action???\"\"\"\n",
    "            q_table[state1][action] = \"\"\"À compléter\"\"\"\n",
    "        else:\n",
    "            \"\"\"Hmmmm... On dirait qu'on va s'amuser avec l'équation de Bellman!!\"\"\"\n",
    "            q_table[state1][action] = \"\"\"À compléter\"\"\"\n",
    "        \n",
    "        # The new state becomes state1 of the next loop\n",
    "        state1 = state2\n",
    "    \n",
    "    # Epsilon decay\n",
    "    epsilon -= epsilon_reduction\n",
    "    \n",
    "    # Display rewards\n",
    "    reward_list.append(total_reward)\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        avg_reward = np.mean(reward_list)\n",
    "        avg_reward_list.append(avg_reward)\n",
    "        reward_list = []\n",
    "        print('Iteration #{} -- Average reward = {}'.format(episode+1, avg_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La mise à l'épreuve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut maintenant voir notre agent en action avec la Policy optimale. Quelle fonction utilisera-t-on pour sélectionner chaque action?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state = obs_to_state(env.reset())\n",
    "done = False\n",
    "while done is False:\n",
    "    env.render()\n",
    "    action = \"\"\"À compléter\"\"\"\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    state = obs_to_state(obs)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évolution de l'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un petit graphique pour voir l'évolution du Return moyen de chaque tranche de 100 épisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(100*(np.arange(len(avg_reward_list)) + 1), avg_reward_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
