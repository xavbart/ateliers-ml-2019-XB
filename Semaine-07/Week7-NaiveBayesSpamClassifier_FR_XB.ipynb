{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRECTION\n",
    "https://nbviewer.jupyter.org/github/fxbabin/ateliers_ml_2019/blob/master/Semaine7/NaiveBayesSpamClassifier.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes pour la classification du spam\n",
    "\n",
    "\n",
    "La classification naïve bayésienne (Naive bayes) est un algorithme de classification probabiliste relativement simple qui convient bien aux données que l'on peut catégoriser.\n",
    "\n",
    "En Machine Learning, les applications courantes de Naive Bayes sont la classification des courriels non sollicités (spam), l'analyse des sentiments (emotion analyses) et la catégorisation des documents (data categorisation). Naive Bayes présente des avantages par rapport aux autres algorithmes de classification couramment utilisés en raison de sa simplicité, de sa rapidité et de sa précision sur de petits ensembles de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "Nous utiliserons les données du dépot d'apprentissage machine de l'UCI qui contient plusieurs commentaires Youtube de vidéos musicales très populaires. Chaque commentaire dans les données a été étiqueté comme spam ou ham (commentaire légitime), et nous utiliserons ces données pour former notre algorithme Naive Bayes pour la classification de spam commentaire youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# Pour la manipulation des données\n",
    "import pandas as pd\n",
    "\n",
    "# Pour les opérations matricielles\n",
    "import numpy as np\n",
    "\n",
    "# Pour utiliser le regexabs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                +447935454150 lovely girl talk to me xxx\n",
       "1          I always end up coming back to this song<br />\n",
       "2       my sister just received over 6,500 new <a rel=...\n",
       "3                                                    Cool\n",
       "4                               Hello I am from Palastine\n",
       "5       Wow this video almost has a billion views! Did...\n",
       "6       Go Sgrout check out my rapping video called Fo...\n",
       "7                                        Almost 1 billion\n",
       "8                    sgrout Aslamu Lykum... From Pakistan\n",
       "9       Eminem is idol for very people in EspaÃ±a and ...\n",
       "10                            Help me get 50 subs please \n",
       "11                                         i love song :)\n",
       "12      Alright ladies, if you like this song, then ch...\n",
       "13      The perfect example of abuse from husbands and...\n",
       "14       The boyfriend was Charlie from the TV show LOST \n",
       "15      <a href=\"https://www.facebook.com/groups/10087...\n",
       "16                  Take a look at this video on YouTube:\n",
       "17                 Check out our Channel for nice Beats!!\n",
       "18           Rihanna and Eminem together are unstoppable.\n",
       "19                    Check out this playlist on YouTube:\n",
       "20      This song is about Rape and Cheating Â  Â Â <b...\n",
       "21                                            like please\n",
       "22      I love this song, can t believe it was 5 years...\n",
       "23      OMG that looks just like a piece of the mirror...\n",
       "24      I shared my first song &quot;I Want You&quot;,...\n",
       "25      Come and check out my music!Im spamming on loa...\n",
       "26                    Check out this playlist on YouTube:\n",
       "27      HUH HYUCK HYUCK IM SPECIAL WHO S WATCHING THIS...\n",
       "28      You exactly who u want to be,watching your fav...\n",
       "29          Rihanna is absolutely gorgeous in this video.\n",
       "                              ...                        \n",
       "1929                               check out my new video\n",
       "1930    Hey Music Fans I really appreciate all of you ...\n",
       "1931    Hello everyone, It Is not my intention to spam...\n",
       "1932    ******* Facebook is LAME and so 2004! Check ou...\n",
       "1933    Please check out and send to others Freedom an...\n",
       "1934    Nice to meet You - this is Johnny: 1. If You a...\n",
       "1935     hey you ! check out the channel of Alvar Lake !!\n",
       "1936    Hi -this is Johnny: 1. If You already know my ...\n",
       "1937                                                  wow\n",
       "1938                                    Love this song!!!\n",
       "1939                                Love this song !!!!!!\n",
       "1940    Check out this video on YouTube:<br />&quot;Th...\n",
       "1941    i watched this because of the large amount of ...\n",
       "1942    O peoples of the earth, I have seen how you pe...\n",
       "1943                 this song always gives me chills! :)\n",
       "1944                                  I love dis song!! 3\n",
       "1945    I WILL NEVER FORGET THIS SONG IN MY LIFE LIKE ...\n",
       "1946    ********OMG Facebook is OLD! Check out  ------...\n",
       "1947    Hey Music Fans I really appreciate all of you ...\n",
       "1948    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
       "1949    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
       "1950    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
       "1951                                  Waka waka she rules\n",
       "1952                             she is sooooo beautiful!\n",
       "1953                                    well done shakira\n",
       "1954    I love this song because we sing it at Camp al...\n",
       "1955    I love this song for two reasons: 1.it is abou...\n",
       "1956                                                  wow\n",
       "1957                              Shakira u are so wiredo\n",
       "1958                           Shakira is the best dancer\n",
       "Name: content, Length: 1959, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger les données du fichier 'YoutubeCommentsSpam.csv' en utilisant pandas\n",
    "data_comments = pd.read_csv('YoutubeCommentsSpam.csv')\n",
    "\n",
    "# Créer des libellés de colonne : \"content\" et \"label\". \n",
    "# conseils : la méthode 'colums' peut être utile \n",
    "data_comments.columns = ['content','label']\n",
    "\n",
    "# Afficher les premières lignes de notre ensemble de données pour s'assurer que la colonne \"label\" a été ajoutée\n",
    "data_comments.head(10)\n",
    "data_comments[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{ATTENTION: Ne regardez pas les liens dans les commentaires, ce sont des spams! ;)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                +447935454150 lovely girl talk to me xxx\n",
      "2       my sister just received over 6,500 new <a rel=...\n",
      "4                               Hello I am from Palastine\n",
      "6       Go Sgrout check out my rapping video called Fo...\n",
      "8                    sgrout Aslamu Lykum... From Pakistan\n",
      "10                            Help me get 50 subs please \n",
      "12      Alright ladies, if you like this song, then ch...\n",
      "15      <a href=\"https://www.facebook.com/groups/10087...\n",
      "16                  Take a look at this video on YouTube:\n",
      "17                 Check out our Channel for nice Beats!!\n",
      "19                    Check out this playlist on YouTube:\n",
      "21                                            like please\n",
      "24      I shared my first song &quot;I Want You&quot;,...\n",
      "25      Come and check out my music!Im spamming on loa...\n",
      "26                    Check out this playlist on YouTube:\n",
      "27      HUH HYUCK HYUCK IM SPECIAL WHO S WATCHING THIS...\n",
      "30      Check out this video on YouTube:<br /><br />Lo...\n",
      "33                    Check out this playlist on YouTube:\n",
      "34                       Check out this video on YouTube:\n",
      "35                       Check out this video on YouTube:\n",
      "38      Check out this playlist on YouTube:chcfcvzfzfb...\n",
      "39                   Check out this playlist on YouTube: \n",
      "40      Im gonna share a little ryhme canibus blows em...\n",
      "41                       Check out this video on YouTube:\n",
      "42      Check out this video on YouTube<br /><br /><br />\n",
      "43        CHECK OUT THE NEW REMIX !!!<br />CLICK CLICK !!\n",
      "44                    Check out this playlist on YouTube:\n",
      "45      I personally have never been in a abusive rela...\n",
      "48                                  plese subscribe to me\n",
      "49                       Check out this video on YouTube:\n",
      "                              ...                        \n",
      "1915             CHECK OUT partyman318 FR GOOD TUNEZ!! :D\n",
      "1916    Hey youtubers... I really appreciate all of yo...\n",
      "1917    Hey Music Fans I really appreciate any of you ...\n",
      "1918    Hey Music Fans I really appreciate any of you ...\n",
      "1919    Hey Music Fans I really appreciate any of you ...\n",
      "1920                   Hi. Check out and share our songs.\n",
      "1921                   Hi. Check out and share our songs.\n",
      "1922                    Hi.Check out and share our songs.\n",
      "1923    Hey Music Fans I really appreciate any of you ...\n",
      "1924    Hey, I am doing the Forty Hour famine so I ll ...\n",
      "1925             Love itt and ppl check out my channel!!!\n",
      "1926                                 SUBSCRIBE MY CHANNEL\n",
      "1927                                       adf.ly / KlD3Y\n",
      "1928                                       adf.ly / KlD3Y\n",
      "1929                               check out my new video\n",
      "1930    Hey Music Fans I really appreciate all of you ...\n",
      "1931    Hello everyone, It Is not my intention to spam...\n",
      "1932    ******* Facebook is LAME and so 2004! Check ou...\n",
      "1933    Please check out and send to others Freedom an...\n",
      "1934    Nice to meet You - this is Johnny: 1. If You a...\n",
      "1935     hey you ! check out the channel of Alvar Lake !!\n",
      "1936    Hi -this is Johnny: 1. If You already know my ...\n",
      "1940    Check out this video on YouTube:<br />&quot;Th...\n",
      "1942    O peoples of the earth, I have seen how you pe...\n",
      "1945    I WILL NEVER FORGET THIS SONG IN MY LIFE LIKE ...\n",
      "1946    ********OMG Facebook is OLD! Check out  ------...\n",
      "1947    Hey Music Fans I really appreciate all of you ...\n",
      "1948    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "1949    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "1950    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "Name: content, Length: 1004, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Afficher les commentaires de spam dans les données\n",
    "# N'ALLEZ PAS SUR LES LIENS !!!!! sérieusement, ce sont des spams.... \n",
    "print(data_comments[\"content\"][data_comments[\"label\"] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En parcourant les commentaires qui ont été étiquetés comme spam dans ces données, il semble que ces commentaires sont soit sans rapport avec la vidéo, soit comme une forme de publicité. L'expression \"check out\" semble être très populaire dans ces commentaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics and Data Cleaning\n",
    "\n",
    "Le tableau ci-dessous montre que cet ensemble de données se compose de $1959$ commentaires youtube, dont environ $49\\%$ sont des commentaires légitimes et environ $51\\%$ sont du spam. Cette grande variation de classes dans notre ensemble de données nous aidera à tester l'exactitude de nos algorithmes sur l'ensemble des données de test. \n",
    "\n",
    "La longueur moyenne de chaque commentaire est d'environ $96$ caractères, ce qui représente environ $15$ mots en moyenne par commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_as_lambda</th>\n",
       "      <th>length_as_len</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>515</td>\n",
       "      <td>515</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>141</td>\n",
       "      <td>141</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>492</td>\n",
       "      <td>492</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>492</td>\n",
       "      <td>492</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>492</td>\n",
       "      <td>492</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1959 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      length_as_lambda  length_as_len  length\n",
       "0                   40             40      40\n",
       "1                   46             46      46\n",
       "2                  200            200     200\n",
       "3                    4              4       4\n",
       "4                   25             25      25\n",
       "5                   73             73      73\n",
       "6                   65             65      65\n",
       "7                   16             16      16\n",
       "8                   36             36      36\n",
       "9                   69             69      69\n",
       "10                  27             27      27\n",
       "11                  14             14      14\n",
       "12                 210            210     210\n",
       "13                 172            172     172\n",
       "14                  48             48      48\n",
       "15                 111            111     111\n",
       "16                  37             37      37\n",
       "17                  38             38      38\n",
       "18                  44             44      44\n",
       "19                  35             35      35\n",
       "20                 118            118     118\n",
       "21                  11             11      11\n",
       "22                  77             77      77\n",
       "23                 126            126     126\n",
       "24                 170            170     170\n",
       "25                 116            116     116\n",
       "26                  35             35      35\n",
       "27                 195            195     195\n",
       "28                  66             66      66\n",
       "29                  45             45      45\n",
       "...                ...            ...     ...\n",
       "1929                22             22      22\n",
       "1930               496            496     496\n",
       "1931               500            500     500\n",
       "1932               145            145     145\n",
       "1933               143            143     143\n",
       "1934               514            514     514\n",
       "1935                48             48      48\n",
       "1936               515            515     515\n",
       "1937                 3              3       3\n",
       "1938                17             17      17\n",
       "1939                21             21      21\n",
       "1940               131            131     131\n",
       "1941               108            108     108\n",
       "1942               476            476     476\n",
       "1943                36             36      36\n",
       "1944                19             19      19\n",
       "1945               105            105     105\n",
       "1946               141            141     141\n",
       "1947               496            496     496\n",
       "1948               492            492     492\n",
       "1949               492            492     492\n",
       "1950               492            492     492\n",
       "1951                19             19      19\n",
       "1952                24             24      24\n",
       "1953                17             17      17\n",
       "1954                58             58      58\n",
       "1955                93             93      93\n",
       "1956                 3              3       3\n",
       "1957                23             23      23\n",
       "1958                26             26      26\n",
       "\n",
       "[1959 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajouter une nouvelle colonne pour la longueur de chaque commentaire\n",
    "# conseils: utiliser map et lambda\n",
    "data_comments['length_as_lambda'] = list(map (lambda x: len(x), data_comments['content']))\n",
    "data_comments['length_as_len'] = list(map (len, data_comments['content']))\n",
    "data_comments['length'] = data_comments['content'].apply(len)\n",
    "\n",
    "# Permet d'afficher un tableau avec plusieurs données statistiques (mean, stdev, min, max)\n",
    "data_comments[[\"label\",\"length_as_lambda\"]].describe()\n",
    "\n",
    "\n",
    "data_comments.loc[:,[\"length_as_lambda\",\"length_as_len\",'length']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour notre algorithme de classification Naive Bayes, nous diviserons les données en deux parties: entrainement et tests. La partie d'entrainement sera utilisé pour former l'algorithme de classification du spam, et l'ensemble de test ne sera utilisé que pour tester sa précision. \n",
    "\n",
    "En général, La partie d'entrainement devrait être plus grand que La partie de test et les deux devraient provenir de la même population (la population dans notre cas est Youtube commentaires pour les vidéos musicales). \n",
    "\n",
    "**Nous sélectionnerons au hasard $75\\%$ des données pour la formation et $25\\%$ des données pour les tests.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  label  length_as_lambda  \\\n",
      "1     I always end up coming back to this song<br />      0                46   \n",
      "2  my sister just received over 6,500 new <a rel=...      1               200   \n",
      "3                                               Cool      0                 4   \n",
      "5  Wow this video almost has a billion views! Did...      0                73   \n",
      "6  Go Sgrout check out my rapping video called Fo...      1                65   \n",
      "\n",
      "   length_as_len  length   uniform  \n",
      "1             46      46  0.393081  \n",
      "2            200     200  0.623970  \n",
      "3              4       4  0.637877  \n",
      "5             73      73  0.299172  \n",
      "6             65      65  0.702198  \n",
      "                                              content  label  \\\n",
      "0            +447935454150 lovely girl talk to me xxx      1   \n",
      "4                           Hello I am from Palastine      1   \n",
      "7                                    Almost 1 billion      0   \n",
      "8                sgrout Aslamu Lykum... From Pakistan      1   \n",
      "13  The perfect example of abuse from husbands and...      0   \n",
      "\n",
      "    length_as_lambda  length_as_len  length   uniform  \n",
      "0                 40             40      40  0.903482  \n",
      "4                 25             25      25  0.880499  \n",
      "7                 16             16      16  0.903206  \n",
      "8                 36             36      36  0.881382  \n",
      "13               172            172     172  0.889215  \n"
     ]
    }
   ],
   "source": [
    "# Séparons les données en 2 groupes ! (75% training, 25% test)\n",
    "\n",
    "# Ceci nous permet d'obtenir la même allocation aléatoire pour chaque série de codes. RTFM if you want ;)\n",
    "np.random.seed(2019)\n",
    "\n",
    "# Ajout d'un vecteur colonne 'uniform' de nombres générés aléatoirement entre 0 et 1 \n",
    "# Astuce : dans numpy, il existe une méthode pour prélever un échantillon à partir d'une distribution uniforme.\n",
    "\n",
    "data_comments[\"uniform\"] = np.random.uniform(0,1, len(data_comments))\n",
    "#print(data_comments['uniform'])\n",
    "\n",
    "#a_list_of_unif = np.random.uniform(0,1, len(data_comments))\n",
    "#a_list_of_unif\n",
    "# Comme le nombre dans notre colonne 'uniform' est distribué uniformément, \n",
    "# environ 75 % de ces chiffres devraient être inférieurs à 0,75 %, prenons ces 75 %.\n",
    "data_comments_train = data_comments[data_comments[\"uniform\"] < 0.75]\n",
    "\n",
    "# Même chose pour les 25 % de ces numéros qui sont supérieurs à 0,75\n",
    "data_comments_test = data_comments[data_comments[\"uniform\"] >= 0.75]\n",
    "\n",
    "print(data_comments_train.head())\n",
    "print(data_comments_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1467.000000\n",
       "mean        0.507157\n",
       "std         0.500119\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         1.000000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vérifiez que les données d'entraînement contiennent à la fois des commentaires de spam et de ham\n",
    "data_comments_train[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    492.000000\n",
       "mean       0.528455\n",
       "std        0.499698\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Même chose pour le data test\n",
    "data_comments_test[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données d'entrainement et de test ont toutes deux un bon mélange de spam et de ham, nous sommes donc prêts à passer à la formation sur le classificateur Naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joindre tout les commentaires dans une seule et même liste\n",
    "# Astuce: 'separator'.join(list)\n",
    "training_list_words = ' '.join(data_comments_train[\"content\"])\n",
    "\n",
    "# Diviser la liste des commentaires en une liste de mots uniques\n",
    "# Astuce: set() and sorted()\n",
    "train_unique_words = set(sorted(str.split(training_list_words)))\n",
    "\n",
    "# Nombre de mots uniques dans le data training\n",
    "vocab_size_train = len(train_unique_words)\n",
    "\n",
    "#print(training_list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in training data: 5491\n",
      "First 5 words in our unique set of words: \n",
      " ['ILove', '/>Doing', 'original', 'gaming', 'Netherland/']\n"
     ]
    }
   ],
   "source": [
    "# Description résumée des commentaires\n",
    "print('Unique words in training data: %s' % vocab_size_train)\n",
    "print('First 5 words in our unique set of words: \\n % s' % list(train_unique_words)[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela devrait ressember à quelquechose comme ça:\n",
    "\n",
    "```Unique words in training data: 5898\n",
    "First 5 words in our unique set of words: \n",
    "['now!!!!!!', 'yellow', 'four', '/>.Pewdiepie', 'Does']```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actuellement, \"now !!\" et \"Now !!!!!!\", ainsi que \"DOES\", \"DoEs\", et \"does\" sont tous considérés comme des mots uniques. Pour la classification du spam, il est probablement préférable de traiter légèrement les données pour en améliorer l'exactitude. Dans notre cas, nous pouvons nous concentrer sur les lettres et les chiffres, ainsi que convertir tous les commentaires en minuscules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanword(a_list_of_word):\n",
    "    a_list_of_word = [ re.sub('[^a-zA-Z0-9]*', '', word) for word in a_list_of_word]\n",
    "    a_list_of_word = set([ word.lower() for word in a_list_of_word])\n",
    "    return a_list_of_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 words in our processed unique set of words: \n",
      " ['herehttpswwwfacebookcomtlouxmusic', 'filled', 'shot', 'behavior', 'huge'] 5491 3481\n",
      "alt First 5 words in our processed unique set of words: \n",
      " ['herehttpswwwfacebookcomtlouxmusic', 'shot', 'filled', 'behavior', 'huge'] 3480\n"
     ]
    }
   ],
   "source": [
    "# Garder uniquement les chiffres et les lettres\n",
    "# Astuce: utiliser regex et les list comprehension\n",
    "#train_unique_words_alt = [ re.sub('\\W*', '', word) for word in train_unique_words]\n",
    "vocab_size_train_before = len(train_unique_words)\n",
    "train_unique_words_alt = cleanword(train_unique_words_alt)\n",
    "train_unique_words = [ re.sub('[^a-zA-Z0-9]*', '', word) for word in train_unique_words]\n",
    "\n",
    "# Convertir toutes les lettres en minuscule\n",
    "# Astuce: set() ?\n",
    "train_unique_words_alt = set([ word.lower() for word in train_unique_words_alt])\n",
    "train_unique_words = set([ word.lower() for word in train_unique_words])\n",
    "# Nombre de mots uniques dans le data training\n",
    "vocab_size_train = len(train_unique_words)\n",
    "\n",
    "# Description résumée des commentaires\n",
    "#print('Unique words in processed training data: %s' % vocab_size_train)\n",
    "print('First 5 words in our processed unique set of words: \\n % s' % list(train_unique_words)[1:6],vocab_size_train_before, vocab_size_train)\n",
    "print('alt First 5 words in our processed unique set of words: \\n % s' % list(train_unique_words_alt)[1:6], len(train_unique_words_alt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for Spam Classification\n",
    "\n",
    "Ok, alors voilà le plan :\n",
    "\n",
    "- Tout d'abord, nous avons séparé nos données de formation en 2 sous-ensembles : training et test.\n",
    "\n",
    "- puis nous allons créer plusieurs fonctions pour vérifier combien de fois chaque mot est apparu dans le spam et non dans les commentaires de spam, \n",
    "    - et vérifier la probabilité que chaque mot apparaisse dans le spam/non spam\n",
    "\n",
    "- alors les 2 fonctions les plus importantes : train() et classify()\n",
    "\n",
    "- Et enfin, nous allons vérifiez l'exactitude de nos prédictions.\n",
    "\n",
    "Passons au code !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainPositive = dict(\\\n",
    "#                     word : count for word in \\\n",
    "#                     set(sorted(str.split(data_comments_train[\"content\"][data_comments_train[\"label\"] == 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On initialise des dictionnaire avec des mots de commentaires comme \"keys\", et leur étiquette comme \"value\".\n",
    "trainPositive = dict()\n",
    "trainNegative = dict()\n",
    "\n",
    "# On initialise ces variables à zéro\n",
    "positiveTotal = 0\n",
    "negativeTotal = 0\n",
    "\n",
    "# Même chose, mais en float ;) \n",
    "pSpam = .0\n",
    "pNotSpam = .0\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#def initialize_dicts():\n",
    "\n",
    "# # On initialise les dictionnaires avec 0 comme valeur \n",
    "for word in train_unique_words:\n",
    "    #print(word)\n",
    "    global trainPositive\n",
    "    global trainNegative\n",
    "    # skip empty words('' and ' ')\n",
    "    if word == '' or word == ' ':\n",
    "        continue # goes to next word\n",
    "    \n",
    "    # Pour le moment, tout est classifier comme ham (légitime) ## NON for the moment nothing is none\n",
    "    trainPositive[word] = 0 #create word\n",
    "    trainNegative[word] = 0\n",
    "print(trainNegative['sgrout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 years and i still dont get the music video help someone? 0\n",
      "0 0 36 0\n"
     ]
    }
   ],
   "source": [
    "# Compter le nombre de fois que le mot dans le commentaire apparaît dans les commentaires de spam et ham\n",
    "def processComment(comment,label):\n",
    "    global positiveTotal\n",
    "    global negativeTotal\n",
    "    global trainNegative\n",
    "    global trainPositive\n",
    "    \n",
    "    # Séparer le commentaire en liste de mots\n",
    "    comment = cleanword(set(sorted(str.split(comment))))\n",
    "    \n",
    "    # Pour chaque mot du commentaire\n",
    "    for word in comment:\n",
    "        print(word)\n",
    "        ##print(type(train_unique_words))\n",
    "        #Checker si le mot est bien dans la base de donnée\n",
    "        if not(word in train_unique_words):\n",
    "            print(\"mot inconnu\")\n",
    "            continue\n",
    "        \n",
    "        #Checker si ce n'est pas un '' ou ' '\n",
    "        if word == '' or word == ' ':\n",
    "            continue\n",
    "        \n",
    "        # Checker si le mot n'est pas du spam (ham)\n",
    "        if label == 0:\n",
    "            trainNegative[word] += 1\n",
    "            positiveTotal = positiveTotal + 1\n",
    "            # Incrémenter le nombre de fois que le mot apparaît dans les commentaires non spam\n",
    "            \n",
    "            \n",
    "        # spam comments\n",
    "        if label == 1:\n",
    "            trainPositive[word] += 1\n",
    "            negativeTotal = negativeTotal + 1\n",
    "            # Incrémenter le nombre de fois que le mot apparaît dans les commentaires spam\n",
    "            \n",
    "            \n",
    "onedata = data_comments_train.loc[158]\n",
    "#data_comments.loc[2:3,[\"label\"]]\n",
    "\n",
    "##processComment(onedata[\"content\"],onedata[\"label\"])\n",
    "print(onedata[\"content\"],onedata[\"label\"])\n",
    "\n",
    "print(trainPositive['help'],trainNegative['someone'],positiveTotal, negativeTotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##METTRE LA FORMULE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, on a la fonction qui va calculer la Prob(word|spam) et Prob(word|ham)\n",
    "def conditionalWord(word,label):\n",
    "   \n",
    "    # Paramètre de lissage de Laplace (Laplace Smoothing)\n",
    "    # Rappel : pour avoir accès à une variable globale à l'intérieur d'une fonction \n",
    "    # vous devez le spécifier en utilisant le mot 'global'.\n",
    "    global positiveTotal\n",
    "    global negativeTotal\n",
    "    \n",
    "    # word in ham comment\n",
    "    if(label == 0):\n",
    "        # Calculer Prob(word|ham)\n",
    "        return trainNegative[word] / negativeTotal\n",
    "    \n",
    "    # word in spam comment\n",
    "    else:\n",
    "        # Calculer Prob(word|ham)\n",
    "        return trainPositive[word] / positiveTotal\n",
    "        \n",
    "       \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, on a la fonction qui va calculer la Prob(spam|comment) or Prob(ham|comment)\n",
    "def conditionalComment(comment,label):\n",
    "    \n",
    "    # On initialise la probabilité conditionelle\n",
    "    prob_label_comment = 1.0\n",
    "    \n",
    "    # On sépare le commentaire en liste de mots\n",
    "    comment = cleanword(set(sorted(str.split(comment))))\n",
    "    \n",
    "    # Pour chaque mot du commentaire\n",
    "    for word in comment:\n",
    "        \n",
    "        # Calculer la valeur de P(label|comment)\n",
    "        # On suppose ici qu'on a une independance conditionnelle (p(A) * p(B))\n",
    "        prob_label_comment = prob_label_comment * conditionalWord(word,label)\n",
    "    \n",
    "    return prob_label_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer plusieurs probabilités conditionnelles dans les données d'entraînement\n",
    "def train():\n",
    "    # Rappel: on aura besoin de pSpam et pNotSpam ici ;) \n",
    "    global pSpam\n",
    "    global pNotSpam\n",
    "\n",
    "    # Initialisation de nos variables: le nombre total de commentaires et le nombre de commentaires de spam \n",
    "    total = 0.0\n",
    "    num_spam = 0.0\n",
    "    #print(data_comments_train.head())\n",
    "    print('Starting training ...')\n",
    "    # Passez en revue chaque commentaire dans les données d'entraînement \n",
    "    for index, row in data_comments_train.iterrows():\n",
    "        #print(type(row))\n",
    "        print(row)\n",
    "        total += 1\n",
    "                \n",
    "       # Vérifiez si le commentaire est du spam ou non (ham)\n",
    "        if row.label == 1: #spam\n",
    "       # Incrémenter les valeurs selon que le commentaire est du spam ou non\n",
    "            num_spam += 1\n",
    "       # Mettre à jour le dictionnaire du spam et ham\n",
    "        \n",
    "        print(\"line is [\",row.content,\"]\\n\")\n",
    "        processComment(row.content,row.label)\n",
    "            \n",
    "        conditionalComment(row.content,row.label)\n",
    "        \n",
    "    # Calcule des probabilitées a priori, P(spam), P(ham)\n",
    "    pSpam = num_spam / total\n",
    "    pNotSpam = 1 - pSpam\n",
    "    print('Training done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n",
      "content             I always end up coming back to this song<br />\n",
      "label                                                            0\n",
      "length_as_lambda                                                46\n",
      "length_as_len                                                   46\n",
      "length                                                          46\n",
      "uniform                                                   0.393081\n",
      "Name: 1, dtype: object\n",
      "line is [ I always end up coming back to this song<br /> ]\n",
      "\n",
      "\n",
      "songbr\n",
      "end\n",
      "coming\n",
      "this\n",
      "i\n",
      "always\n",
      "up\n",
      "back\n",
      "to\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-8ad826f8225e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lancer notre fonction train de Naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-186-e4b76174d0b5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprocessComment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mconditionalComment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Calcule des probabilitées a priori, P(spam), P(ham)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-185-a59c93a2fe67>\u001b[0m in \u001b[0;36mconditionalComment\u001b[0;34m(comment, label)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Calculer la valeur de P(label|comment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# On suppose ici qu'on a une independance conditionnelle (p(A) * p(B))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mprob_label_comment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_label_comment\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconditionalWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprob_label_comment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-184-6afbf7cda1a8>\u001b[0m in \u001b[0;36mconditionalWord\u001b[0;34m(word, label)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Calculer Prob(word|ham)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainNegative\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnegativeTotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# word in spam comment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "# Lancer notre fonction train de Naive Bayes\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier les commentaires sont du spam ou ham\n",
    "def classify(comment):\n",
    "    \n",
    "    # get global variables\n",
    "    \n",
    "    \n",
    "    # Calculer la valeur proportionnelle à Pr(comment|ham)\n",
    "    isNegative = \n",
    "    \n",
    "    # Calculer la valeur proportionnelle à Pr(comment|spam)\n",
    "    isPositive = \n",
    "    \n",
    "    # Output -> True = spam, False = ham en fonction des 2 variables calculées précédemment (il faut comparer les variables)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser la prédiction du spam dans les données de test\n",
    "prediction_test = []\n",
    "\n",
    "# Obtenez la précision des prédictions sur les données d'essai\n",
    "for ...\n",
    "\n",
    "    # ajouter un commentaire classifié à la liste prediction_test \n",
    "    \n",
    "\n",
    "# Checker la précision: \n",
    "# D'abord le nombre de prédictions correctes \n",
    "correct_labels = \n",
    "# Ensuite la moyenne des prédictions correctes\n",
    "test_accuracy = \n",
    "\n",
    "#print prediction_test\n",
    "print(\"Proportion of comments classified correctly on test set: %s\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons d'écrire quelques commentaires pour voir s'ils sont classés comme spam ou ham. \n",
    "\n",
    "Rappelez-vous que le \"True\" est pour les commentaires de spam, et \"False\" est pour les commentaires ham. \n",
    "Essayez vous même !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam\n",
    "classify(\"Guys check out my new chanell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam\n",
    "classify(\"I have solved P vs. NP, check my video https://www.youtube.com/watch?v=dQw4w9WgXcQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham\n",
    "classify(\"I liked the video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham\n",
    "classify(\"Its great that this video has so many views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour aller plus loin...\n",
    "## Extending Bag of Words by Using TF-IDF\n",
    "\n",
    "Jusqu'à présent, nous avons utilisé le modèle du Bag of Words pour représenter les commentaires en tant que vecteurs. Le \"Bag of Words\" est une liste de tous les mots uniques trouvés dans les données training, alors chaque commentaire peut être représenté par un vecteur qui contient la fréquence de chaque mot unique qui apparaît dans le commentaire.\n",
    "\n",
    "Par exemple, si les données training contiennent les mots $(hi, how, how, my, grade, are, you),$ alors le texte \"how are you you\" peut être représenté par $(0,1,0,0,0,1,2).$ La principale raison pour laquelle nous faisons cela dans notre application est que les commentaires peuvent varier en longueur, mais la longueur des mots uniques reste fixe.\n",
    "\n",
    "Dans notre contexte, le TF-IDF est une mesure de l'importance d'un mot dans un commentaire par rapport à tous les mots de nos données de formation. Par exemple, si un mot tel que \"the\" apparaissait dans la plupart des commentaires, le TF-IDF serait petit car ce mot ne nous aide pas à faire la différence entre les commentaires spam et ham. Notez que \"TF\" signifie \"Term Frequency\" et \"IDF\" signifie \"Inverse Document Frequency\".\n",
    "\n",
    "En particulier, \"TF\" indiqué par $tf(w,c)$ est le nombre de fois que le mot $w$ apparaît dans le commentaire donné $c$. Alors que \"IDF\" est une mesure de la quantité d'informations qu'un mot donné fournit pour différencier les commentaires. PLus précisement, $IDF$ est formulé comme ceci:\n",
    "\n",
    "\n",
    ">$idf(w, D) = log(\\frac{\\text{Number of comments in train data $D$}}{\\text{Number of comments containing the word $w$}}).$ \n",
    "\n",
    "\n",
    "Pour combiner \"TF\" et \"IDF\" ensemble, nous prenons simplement le produit, donc:\n",
    "\n",
    "\n",
    ">$$TFIDF = tf(w,c) \\times idf(w, D) = (\\text{Number of times $w$ appears in comment $c$})\\times log(\\frac{\\text{Number of comments in train data $D$}}{\\text{Number of comments containing the word $w$}}).$$\n",
    "\n",
    "\n",
    "Maintenant, le $TF-IDF$ peut être utilisé pour pondérer les vecteurs qui résultent de l'approche \"Bag of Words\".\n",
    "\n",
    "Par exemple, supposons qu'un commentaire contienne \"ceci\" 2 fois, donc $tf = 2$. \n",
    "Si nous avions alors 1000 commentaires dans nos données de formation, et que le mot \"ceci\" apparaît dans 100 commentaires, $idf = log(1000/100) = 2.$. \n",
    "\n",
    "Par conséquent, dans cet exemple, le poids TF-IDF serait de $2*2 = 4$ pour le mot \"ceci\" apparaît deux fois dans un commentaire particulier. Pour incorporer TF-IDF dans le réglage des baies naïves, nous pouvons calculer :\n",
    "\n",
    ">$$Pr(word|spam) = \\frac{\\sum_{\\text{c is spam}}TFIDF(word,c,D)}{\\sum_{\\text{word in spam c}}\\sum_{\\text{c is spam}}TFIDF(word,c,D)+ \\text{Number of unique words in data}},$$ \n",
    "\n",
    ">where $TFIDF(word,c,D) = TF(word,c) \\times IDF(word,data).$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer TFIDF(word, comment, data)\n",
    "def TFIDF(comment, train):\n",
    "    \n",
    "    # Diviser le commentaire en une liste de mot\n",
    "    comment = \n",
    "    \n",
    "    # Initiailiser tf-idf selon la longueur du commentaire\n",
    "    tfidf_comment = \n",
    "    \n",
    "    # Initiailiser nombre de commentaires contenant un mot\n",
    "    num_comment_word = 0\n",
    "    \n",
    "    # Initialiser l'index pour les mots dans le commentaire\n",
    "    word_index = 0\n",
    "    \n",
    "    # Pour chaque mot du commentaire\n",
    "    for...\n",
    "        \n",
    "        # Calculer la fréquence des termes (tf)\n",
    "        # Compter la fréquence du mot dans les commentaires\n",
    "        tf = \n",
    "        \n",
    "        # Trouver le nombre de commentaires contenant un mot\n",
    "        for ...\n",
    "            \n",
    "            # Incrémenter le compteur de mots si le mot trouvé dans le commentaire\n",
    "            if ...\n",
    "        \n",
    "        # Calculer la fréquence du document inverse (idf)\n",
    "        # log(Nombre total de commentaires/nombre de commentaires avec mot)\n",
    "        idf = \n",
    "        \n",
    "        # Mettre a jour le poids tf-idf du mot\n",
    "        \n",
    "        \n",
    "        # Réinitialiser le nombre de commentaires contenant un mot\n",
    "        \n",
    "        \n",
    "        # Passer au mot suivant dans le commentaire\n",
    "        \n",
    "        \n",
    "    return tfidf_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF(\"Check out my new music video plz\",data_comments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et maintenant, implémente TFIDF avec ta fonction de classification\n",
    "# Have fun :D\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
